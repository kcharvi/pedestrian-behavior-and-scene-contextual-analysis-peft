"""
Pedestrian Behavior Metrics Analysis & Visualization (Step 3)

Description:
    This script serves as the final analysis stage for the pedestrian behavior
    evaluation pipeline. It takes the raw, aggregated data CSV generated by
    Step 2 and produces a comprehensive suite of visualizations
    and detailed markdown reports.

    The analysis covers several key areas:
    -   Basic dataset statistics and coverage.
    -   Detailed classification metrics (precision, recall, F1-score) for each
        behavioral attribute.
    -   In-depth performance analysis, including latency, throughput, and scalability
        with respect to the number of pedestrians.
    -   Model confidence and uncertainty analysis to evaluate prediction reliability.
    -   Advanced analysis of the PEFT adapters, focusing on parameter and
        computational efficiency trade-offs.
    -   Spatial and temporal analysis of specific behaviors, such as looking
        and crossing patterns.

Inputs:
    -   Raw Pedestrian Behavior Data: Path specified by `--csv`.
        (Default: `raw_data_pedestrian_behavior_step2.csv`)

Outputs:
    -   A directory named `Pedestrian_Behavior_Analysis_Plots/` containing all
        generated plots (.png) and markdown reports (.md).

Command-Line Arguments:
    -   `--csv` (str, optional):
        Specifies the path to the raw inference data CSV file generated by
        the `inference_pedestrian_behavior_step2.py` script.
"""
from __future__ import annotations

import ast
import os
from pathlib import Path
from typing import List

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import argparse

# =============================================================================
# A. CONFIGURATION & SETUP
# =============================================================================
sns.set_theme(style="whitegrid", context="paper", font_scale=1.1)
COLORS = ["#004488", "#DDAA33", "#BB5566", "#FF5733"]
ATTRS = ["action", "look", "cross", "occlusion"]
PLOTS_DIR = Path("Pedestrian_Behavior_Analysis_Plots")

# =============================================================================
# B. DATA LOADING & PREPARATION
# =============================================================================

def _eval_list(x):
    """Convert string representation to list, return empty list for NaN or malformed data."""

    if isinstance(x, list):
        return x
    if pd.isna(x) or str(x).lower() == "nan":
        return []
    try:
        return ast.literal_eval(str(x))
    except (ValueError, SyntaxError):
        return []

def load_data(csv_path: str) -> pd.DataFrame:
    """Load raw CSV data and add helper columns for analysis."""

    df = pd.read_csv(csv_path, converters={"pred_bbox": _eval_list, "gt_bbox": _eval_list})

    for attr in ATTRS:
        df[f"correct_{attr}"] = df[f"pred_{attr}"] == df[f"gt_{attr}"]

    for prefix in ["pred", "gt"]:
        df[f"{prefix}_cx"] = df[f"{prefix}_bbox"].apply(
            lambda b: (b[0] + b[2]) / 2 if len(b) == 4 else np.nan)
        df[f"{prefix}_cy"] = df[f"{prefix}_bbox"].apply(
            lambda b: (b[1] + b[3]) / 2 if len(b) == 4 else np.nan)
    return df

# =============================================================================
# C. ANALYSIS & REPORT GENERATION
# =============================================================================

# =============================================================================
# C1: Basic Stats
# =============================================================================

def basic_stats(df: pd.DataFrame):
    """Generates a markdown report with basic dataset coverage statistics."""

    total_videos = df["video_id"].nunique()
    total_frames = df[["video_id", "frame_id"]].drop_duplicates().shape[0]
    total_tracks = df[["video_id", "inference_track_id"]].drop_duplicates().shape[0]

    with open(PLOTS_DIR / "00_basic_stats.md", "w") as f:
        f.write("# Dataset Coverage\n\n")
        f.write(f"* Videos processed: **{total_videos}**\n")
        f.write(f"* Unique frames: **{total_frames}**\n")
        f.write(f"* Inference tracks: **{total_tracks}**\n")
    print("   - Basic stats report generated: 00_basic_stats.md")

# =============================================================================
# C2: Detailed Classification Metrics
# =============================================================================

def detailed_classification_metrics(df: pd.DataFrame):
    """Generates a detailed markdown report with per-class classification metrics."""
    with open(PLOTS_DIR / "01_classification_metrics.md", "w") as f:
        f.write("# Detailed Classification Metrics\n\n")
        f.write("Per-class precision, recall, and F1-score for each pedestrian attribute.\n\n")
        
        for attr in ATTRS:
            sub = df.dropna(subset=[f"gt_{attr}", f"pred_{attr}"])
            if sub.empty:
                f.write(f"## {attr.upper()} Attribute\n")
                f.write("No valid data available for this attribute.\n\n")
                continue
            
            report = classification_report(
                sub[f"gt_{attr}"], 
                sub[f"pred_{attr}"], 
                output_dict=True,
                zero_division=0
            )
            
            f.write(f"## {attr.upper()} Attribute\n\n")
            
            f.write("| Class | Precision | Recall | F1-Score | Support |\n")
            f.write("|-------|-----------|--------|----------|----------|\n")
            
            classes = [k for k in report.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]
            classes.sort()
            
            for class_name in classes:
                metrics = report[class_name]
                f.write(f"| {class_name} | {metrics['precision']:.3f} | {metrics['recall']:.3f} | {metrics['f1-score']:.3f} | {int(metrics['support'])} |\n")
            
            f.write("\n### Summary Metrics\n\n")
            f.write("| Metric | Precision | Recall | F1-Score |\n")
            f.write("|--------|-----------|--------|----------|\n")
            f.write(f"| Macro Average | {report['macro avg']['precision']:.3f} | {report['macro avg']['recall']:.3f} | {report['macro avg']['f1-score']:.3f} |\n")
            f.write(f"| Weighted Average | {report['weighted avg']['precision']:.3f} | {report['weighted avg']['recall']:.3f} | {report['weighted avg']['f1-score']:.3f} |\n")
            f.write(f"| **Overall Accuracy** | | | **{report['accuracy']:.3f}** |\n\n")
            
            f.write("---\n\n")
    
    print("   - Detailed classification metrics report generated: 01_classification_metrics.md")

# =============================================================================
# C3: Comprehensive Performance Analysis
# =============================================================================

def comprehensive_performance_analysis(df: pd.DataFrame):
    """Generates a markdown report with comprehensive performance metrics."""
    with open(PLOTS_DIR / "02_performance_analysis.md", "w") as f:
        f.write("# Comprehensive Performance Analysis\n\n")
        f.write("Detailed computational efficiency and model performance metrics.\n\n")
        
        # =====================================================================
        # 1. COMPUTATIONAL EFFICIENCY METRICS
        # =====================================================================
        f.write("## 1. Computational Efficiency Metrics\n\n")
        
        
        total_samples = len(df)
        mean_latency = df['total_frame_time_ms'].mean()
        median_latency = df['total_frame_time_ms'].median()
        p95_latency = df['total_frame_time_ms'].quantile(0.95)
        p99_latency = df['total_frame_time_ms'].quantile(0.99)
        
        mean_fps = df['frame_fps'].mean()
        median_fps = df['frame_fps'].median()
        samples_per_second = 1000 / mean_latency if mean_latency > 0 else 0
        
        f.write("### Latency & Throughput\n\n")
        f.write("| Metric | Value | Unit |\n")
        f.write("|--------|-------|------|\n")
        f.write(f"| Total Samples Processed | {total_samples:,} | samples |\n")
        f.write(f"| Mean Latency | {mean_latency:.2f} | ms/sample |\n")
        f.write(f"| Median Latency | {median_latency:.2f} | ms/sample |\n")
        f.write(f"| 95th Percentile Latency | {p95_latency:.2f} | ms/sample |\n")
        f.write(f"| 99th Percentile Latency | {p99_latency:.2f} | ms/sample |\n")
        f.write(f"| Mean Throughput | {samples_per_second:.1f} | samples/sec |\n")
        f.write(f"| Mean FPS | {mean_fps:.1f} | frames/sec |\n")
        f.write(f"| Median FPS | {median_fps:.1f} | frames/sec |\n\n")
        
        # =====================================================================
        # 2. SCALABILITY ANALYSIS
        # =====================================================================
        f.write("## 2. Scalability Analysis\n\n")
        
        ped_scaling = df.groupby('num_pedestrians_in_frame').agg({
            'total_frame_time_ms': ['mean', 'std', 'count'],
            'frame_fps': ['mean', 'std']
        }).round(2)
        
        f.write("### Performance vs Pedestrian Count\n\n")
        f.write("| Pedestrians | Avg Latency (ms) | Std Latency | Avg FPS | Std FPS | Sample Count |\n")
        f.write("|-------------|------------------|-------------|---------|---------|---------------|\n")
        
        for ped_count in sorted(ped_scaling.index):
            if ped_scaling.loc[ped_count, ('total_frame_time_ms', 'count')] >= 5:
                avg_lat = ped_scaling.loc[ped_count, ('total_frame_time_ms', 'mean')]
                std_lat = ped_scaling.loc[ped_count, ('total_frame_time_ms', 'std')]
                avg_fps = ped_scaling.loc[ped_count, ('frame_fps', 'mean')]
                std_fps = ped_scaling.loc[ped_count, ('frame_fps', 'std')]
                count = int(ped_scaling.loc[ped_count, ('total_frame_time_ms', 'count')])
                f.write(f"| {ped_count} | {avg_lat:.2f} | {std_lat:.2f} | {avg_fps:.1f} | {std_fps:.1f} | {count:,} |\n")
        
        # =====================================================================
        # 3. ACCURACY-EFFICIENCY TRADE-OFFS
        # =====================================================================
        f.write("\n## 3. Accuracy-Efficiency Trade-offs\n\n")
        
        accuracy_metrics = {}
        for attr in ATTRS:
            valid_data = df.dropna(subset=[f"gt_{attr}", f"pred_{attr}"])
            if not valid_data.empty:
                accuracy = (valid_data[f"pred_{attr}"] == valid_data[f"gt_{attr}"]).mean()
                accuracy_metrics[attr] = accuracy
        
        f.write("### Attribute Performance Summary\n\n")
        f.write("| Attribute | Accuracy | Avg Latency (ms) | Efficiency Score* |\n")
        f.write("|-----------|----------|------------------|-------------------|\n")
        
        for attr in ATTRS:
            if attr in accuracy_metrics:
                acc = accuracy_metrics[attr]
                eff_score = acc / mean_latency * 1000 if mean_latency > 0 else 0
                f.write(f"| {attr.upper()} | {acc:.3f} | {mean_latency:.2f} | {eff_score:.2f} |\n")
        
        f.write("\n*Efficiency Score = (Accuracy / Latency) × 1000 (higher is better)\n\n")
        
        # =====================================================================
        # 4. INFERENCE STABILITY ANALYSIS
        # =====================================================================
        f.write("## 4. Inference Stability Analysis\n\n")
        
        latency_cv = df['total_frame_time_ms'].std() / df['total_frame_time_ms'].mean()
        fps_cv = df['frame_fps'].std() / df['frame_fps'].mean()
        
        f.write("### Stability Metrics\n\n")
        f.write("| Metric | Value | Interpretation |\n")
        f.write("|--------|-------|----------------|\n")
        f.write(f"| Latency CV* | {latency_cv:.3f} | {'Stable' if latency_cv < 0.2 else 'Moderate' if latency_cv < 0.5 else 'Unstable'} |\n")
        f.write(f"| FPS CV* | {fps_cv:.3f} | {'Stable' if fps_cv < 0.2 else 'Moderate' if fps_cv < 0.5 else 'Unstable'} |\n")
        f.write(f"| Latency Range | {df['total_frame_time_ms'].min():.1f} - {df['total_frame_time_ms'].max():.1f} ms | - |\n")
        f.write(f"| FPS Range | {df['frame_fps'].min():.1f} - {df['frame_fps'].max():.1f} | - |\n\n")
        f.write("*CV = Coefficient of Variation (std/mean). Lower values indicate more stable performance.\n\n")
        
        # =====================================================================
        # 5. Performance Optimization Recommendations
        # =====================================================================
        f.write("## 5. Performance Optimization Recommendations\n\n")
        
        if mean_latency > 100:
            f.write("- (High Alert) High Latency Detected (>100ms): Consider model quantization, pruning, or batch processing strategies.\n")
        elif mean_latency > 50:
            f.write("- (Moderate) Moderate Latency (50-100ms): Monitor for real-time applications and consider caching.\n")
        else:
            f.write("- (Good) Good Latency Performance (<50ms): Suitable for real-time applications.\n")
        
        if accuracy_metrics:
            best_attr = max(accuracy_metrics.items(), key=lambda x: x[1])
            worst_attr = min(accuracy_metrics.items(), key=lambda x: x[1])
            f.write(f"- (Stat) Best Performing Attribute: {best_attr[0].upper()} ({best_attr[1]:.3f} accuracy)\n")
            f.write(f"- (Stat) Needs Improvement: {worst_attr[0].upper()} ({worst_attr[1]:.3f} accuracy)\n\n")
    
    print("   - Comprehensive performance report generated: 02_performance_analysis.md")

# =============================================================================
# C4: Model Confidence Analysis
# =============================================================================

def model_confidence_analysis(df: pd.DataFrame):
    """Generates a markdown report on model confidence and uncertainty."""

    with open(PLOTS_DIR / "03_confidence_analysis.md", "w") as f:
        f.write("# Model Confidence Analysis\n\n")
        f.write("Uncertainty quantification and confidence score analysis for pedestrian behavior prediction.\n\n")
        
        available_conf_cols = [f"pred_{attr}_confidence" for attr in ATTRS if f"pred_{attr}_confidence" in df.columns]
        if not available_conf_cols:
            f.write("(Warning) No confidence scores found in dataset. Update inference script to extract confidence scores.\n")
            return
        
        # =====================================================================
        # 1. CONFIDENCE DISTRIBUTION ANALYSIS
        # =====================================================================
        f.write("## 1. Confidence Score Distributions\n\n")
        
        for attr in ATTRS:
            conf_col = f"pred_{attr}_confidence"
            if conf_col not in df.columns:
                continue
                
            valid_conf = df[conf_col].dropna()
            if valid_conf.empty:
                continue
                
            mean_conf = valid_conf.mean()
            std_conf = valid_conf.std()
            median_conf = valid_conf.median()
            min_conf = valid_conf.min()
            max_conf = valid_conf.max()
            
            f.write(f"### {attr.upper()} Attribute\n\n")
            f.write("| Metric | Value |\n")
            f.write("|--------|-------|\n")
            f.write(f"| Mean Confidence | {mean_conf:.3f} |\n")
            f.write(f"| Std Confidence | {std_conf:.3f} |\n")
            f.write(f"| Min Confidence | {min_conf:.3f} |\n")
            f.write(f"| Max Confidence | {max_conf:.3f} |\n")
            f.write(f"| Samples | {len(valid_conf):,} |\n\n")

        # =====================================================================
        # 2. CONFIDENCE-BASED FILTERING RECOMMENDATIONS
        # =====================================================================
        f.write("## 2. Confidence-Based Filtering Recommendations\n\n")
        for attr in ATTRS:
            conf_col = f"pred_{attr}_confidence"
            if conf_col in df.columns and not df.dropna(subset=[conf_col, f"gt_{attr}", f"pred_{attr}"]).empty:
                best_threshold, best_score = 0.5, 0
                valid_data = df.dropna(subset=[conf_col, f"gt_{attr}", f"pred_{attr}"])
                for thresh in np.arange(0.3, 0.95, 0.05):
                    filtered = valid_data[valid_data[conf_col] >= thresh]
                    if len(filtered) >= 10:
                        acc = (filtered[f"pred_{attr}"] == filtered[f"gt_{attr}"]).mean()
                        coverage = len(filtered) / len(valid_data)
                        score = acc * (coverage ** 0.5)
                        if score > best_score:
                            best_score, best_threshold = score, thresh
                
                final_filtered = valid_data[valid_data[conf_col] >= best_threshold]
                final_acc = (final_filtered[f"pred_{attr}"] == final_filtered[f"gt_{attr}"]).mean()
                final_coverage = len(final_filtered) / len(valid_data)
                f.write(f"- **{attr.upper()}**: Optimal threshold >= {best_threshold:.2f} -> {final_acc:.3f} accuracy at {final_coverage:.1%} coverage.\n")
    
    print("   - Model confidence analysis report generated: 03_confidence_analysis.md")

# =============================================================================
# C5: Advanced Adapter Analysis
# =============================================================================     

def advanced_adapter_analysis(df: pd.DataFrame):
    """Generates a markdown report with advanced, adapter-specific performance metrics."""
    with open(PLOTS_DIR / "04_advanced_adapter_metrics.md", "w") as f:
        f.write("# Advanced Adapter Performance Metrics\n\n")
        f.write("Analysis of adapter-specific performance, focusing on parameter and computational efficiency.\n\n")
        
        adapter_estimates = {
            'lora': 443906, 'adalora': 444194, 'ia3': 84482, 'loha': 886274, 'lokr': 40706
        }
        adapter_mapping = {
            'action': 'loha', 'look': 'loha', 'cross': 'adalora', 'occlusion': 'adalora'
        }
        
        # =====================================================================
        # 1. PARAMETER EFFICIENCY ANALYSIS
        # =====================================================================
        f.write("## 1. Parameter Efficiency Comparison\n\n")
        f.write("| Attribute | Adapter Type | Est. Params (M) | Accuracy | Efficiency Score* |\n")
        f.write("|-----------|--------------|-----------------|----------|-------------------|\n")
        
        comparison_data = []
        for attr, adapter_type in adapter_mapping.items():
            valid_data = df.dropna(subset=[f"gt_{attr}", f"pred_{attr}"])
            if not valid_data.empty:
                accuracy = (valid_data[f"pred_{attr}"] == valid_data[f"gt_{attr}"]).mean()
                params = adapter_estimates[adapter_type]
                efficiency = accuracy / (params / 1e6)
                f.write(f"| {attr.upper()} | {adapter_type.upper()} | {params/1e6:.2f} | {accuracy:.3f} | {efficiency:.2f} |\n")
                comparison_data.append({'attribute': attr, 'adapter': adapter_type, 'accuracy': accuracy, 'efficiency': efficiency})

        f.write("\n*Efficiency Score = Accuracy / Million Parameters (higher is better)\n\n")
        
        # =====================================================================
        # 2. COMPUTATIONAL COMPLEXITY ANALYSIS
        # =====================================================================
        f.write("## 2. Computational Complexity Analysis\n\n")
        
        gflop_cols = [col for col in df.columns if col.startswith('gflops_')]
        if gflop_cols:
            f.write("### GFLOPs per Adapter\n\n")
            f.write("| Adapter | Mean GFLOPs | Samples |\n")
            f.write("|---------|-------------|----------|\n")
            
            for col in gflop_cols:
                attr_name = col.replace('gflops_', '').upper()
                valid_data = df[col].dropna()
                if not valid_data.empty:
                    mean_gflops = valid_data.mean()
                    samples = len(valid_data)
                    f.write(f"| {attr_name} | {mean_gflops:.3f} | {samples:,} |\n")
        
        # =====================================================================
        # 3. SCALABILITY METRICS
        # =====================================================================
        f.write("\n## 3. Scalability Analysis\n\n")
        
        f.write("### Batch Processing Efficiency\n\n")
        
        if 'num_pedestrians_in_frame' in df.columns:
            batch_analysis = df.groupby('num_pedestrians_in_frame').agg({
                'total_frame_time_ms': ['mean', 'std', 'count'],
                'frame_fps': 'mean'
            }).round(2)
            
            f.write("| Batch Size | Avg Time (ms) | Time per Sample (ms) | Efficiency Gain* | Samples |\n")
            f.write("|------------|---------------|----------------------|------------------|----------|\n")
            
            baseline_time_per_sample = None
            for batch_size in sorted(batch_analysis.index):
                if batch_analysis.loc[batch_size, ('total_frame_time_ms', 'count')] >= 5:
                    avg_time = batch_analysis.loc[batch_size, ('total_frame_time_ms', 'mean')]
                    time_per_sample = avg_time / batch_size if batch_size > 0 else avg_time
                    count = int(batch_analysis.loc[batch_size, ('total_frame_time_ms', 'count')])
                    
                    if baseline_time_per_sample is None:
                        baseline_time_per_sample = time_per_sample
                        efficiency_gain = 1.0
                    else:
                        efficiency_gain = baseline_time_per_sample / time_per_sample
                    
                    f.write(f"| {batch_size} | {avg_time:.1f} | {time_per_sample:.1f} | {efficiency_gain:.2f}× | {count:,} |\n")
            
            f.write("\n*Efficiency Gain = Baseline Time per Sample / Current Time per Sample\n\n")
        
        # =====================================================================
        # 4. ADAPTER COMPARISON SUMMARY
        # =====================================================================
        f.write("## 4. Adapter Type Comparison Summary\n\n")
        
        f.write("### Performance vs Efficiency Trade-offs\n\n")
        
        accuracy_metrics = {}
        for attr in ATTRS:
            valid_data = df.dropna(subset=[f"gt_{attr}", f"pred_{attr}"])
            if not valid_data.empty:
                accuracy = (valid_data[f"pred_{attr}"] == valid_data[f"gt_{attr}"]).mean()
                accuracy_metrics[attr] = accuracy
        
        comparison_data = []
        for attr in ATTRS:
            if attr in accuracy_metrics and attr in adapter_mapping:
                adapter_type = adapter_mapping[attr]
                accuracy = accuracy_metrics[attr]
                params = adapter_estimates[adapter_type]
                
                comparison_data.append({
                    'attribute': attr,
                    'adapter': adapter_type,
                    'accuracy': accuracy,
                    'params': params,
                    'efficiency': accuracy / (params / 1e6)
                })
        
        if comparison_data:
            comparison_data.sort(key=lambda x: x['efficiency'], reverse=True)
            
            f.write("| Rank | Attribute | Adapter | Accuracy | Parameters | Efficiency |\n")
            f.write("|------|-----------|---------|----------|------------|------------|\n")
            
            for i, data in enumerate(comparison_data, 1):
                f.write(f"| {i} | {data['attribute'].upper()} | {data['adapter'].upper()} | "
                       f"{data['accuracy']:.3f} | {data['params']/1e6:.2f}M | {data['efficiency']:.2f} |\n")
        
        # =====================================================================
        # 4. RECOMMENDATIONS
        # =====================================================================
        f.write("\n## 4. Recommendations\n\n")
        if comparison_data:
            best_performer = max(comparison_data, key=lambda x: x['accuracy'])
            most_efficient = max(comparison_data, key=lambda x: x['efficiency'])
            
            f.write(f"- (Best) Best Accuracy: **{best_performer['attribute'].upper()}** ({best_performer['adapter'].upper()}) at {best_performer['accuracy']:.3f}\n")
            f.write(f"- (Most Efficient) Most Efficient: **{most_efficient['attribute'].upper()}** ({most_efficient['adapter'].upper()}) at {most_efficient['efficiency']:.2f} acc/M_params\n")

    print("   - Advanced adapter analysis report generated: 04_advanced_adapter_metrics.md")

# =============================================================================
# D. VISUALIZATION GENERATION
# =============================================================================

# =============================================================================
# D1: Latency & Scalability Analysis
# =============================================================================

def plot_latency_and_scalability(df: pd.DataFrame):
    """Generates and saves latency distribution and scalability plots."""

    fig, ax = plt.subplots(figsize=(10, 6))
    
    latency_data = df["total_frame_time_ms"].dropna()
    mean_lat = latency_data.mean()
    median_lat = latency_data.median()
    std_lat = latency_data.std()
    p95_lat = latency_data.quantile(0.95)
    p99_lat = latency_data.quantile(0.99)
    
    sns.histplot(data=latency_data, ax=ax, bins=40, stat="count", element="step", 
                 fill=True, alpha=0.6, color=COLORS[0], linewidth=2.5, 
                 label='Frame Count Distribution')
    
    ax.axvline(mean_lat, color=COLORS[2], linestyle='--', linewidth=2.5, 
               label=f'Mean: {mean_lat:.1f}ms', alpha=0.9)
    ax.axvline(median_lat, color=COLORS[1], linestyle='--', linewidth=2.5, 
               label=f'Median: {median_lat:.1f}ms', alpha=0.9)
    ax.axvline(p95_lat, color=COLORS[3], linestyle=':', linewidth=2, 
               label=f'95th percentile: {p95_lat:.1f}ms', alpha=0.8)
    
    ax.set_xlabel('Inference Latency per Frame (ms)', fontsize=13, fontweight='bold')
    ax.set_ylabel('Number of Frames', fontsize=13, fontweight='bold')
    ax.set_title('Inference Latency Distribution Analysis', fontsize=15, fontweight='bold', pad=20)
    
    stats_text = f'Statistics (n={len(latency_data):,}):\n' \
                 f'Mean ± SD: {mean_lat:.1f} ± {std_lat:.1f} ms\n' \
                 f'Median [IQR]: {median_lat:.1f} [{latency_data.quantile(0.25):.1f}-{latency_data.quantile(0.75):.1f}] ms\n' \
                 f'95th/99th percentile: {p95_lat:.1f}/{p99_lat:.1f} ms\n' \
                 f'Range: {latency_data.min():.1f} - {latency_data.max():.1f} ms'
    
    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', horizontalalignment='right',
            bbox=dict(boxstyle='round,pad=0.6', facecolor='white', alpha=0.95, 
                     edgecolor='gray', linewidth=1))
    
    ax.legend(loc='lower left', bbox_to_anchor=(0.70, 0.50), frameon=True, 
              fancybox=True, shadow=True, fontsize=10, borderpad=1)
    ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color('gray')
    ax.spines['bottom'].set_color('gray')
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "plot_latency_distribution_enhanced.png", dpi=300, bbox_inches='tight')
    plt.close()

    plt.figure(figsize=(5, 4))
    sns.scatterplot(x="num_pedestrians_in_frame", y="frame_fps", data=df,
                    alpha=0.5, color=COLORS[1])
    sns.lineplot(x="num_pedestrians_in_frame",
                 y="frame_fps", data=df.groupby("num_pedestrians_in_frame")["frame_fps"].mean().reset_index(),
                 color=COLORS[2])
    plt.xlabel("Pedestrians in frame")
    plt.ylabel("FPS")
    plt.title("Scalability: FPS vs. Pedestrian Count")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "plot_fps_vs_peds.png", dpi=300)
    plt.close()
    print("   - Latency & scalability plots saved.")

# =============================================================================
# D2: Adapter Performance Visualizations
# =============================================================================

def plot_adapter_performance_visualizations(df: pd.DataFrame):
    """Generates and saves visualizations for adapter-specific performance."""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 12))
    
    adapter_data = {
        'ACTION (LoHA)': {'params': 0.89}, 'LOOK (LoHA)': {'params': 0.89}, 
        'CROSS (AdaLoRA)': {'params': 0.44}, 'OCCLUSION (AdaLoRA)': {'params': 0.44}
    }
    accuracies = {
        attr: (df[f"pred_{attr}"] == df[f"gt_{attr}"]).mean()
        for attr in ATTRS if not df.dropna(subset=[f"gt_{attr}", f"pred_{attr}"]).empty
    }
    
    names = list(adapter_data.keys())
    params = [v['params'] for v in adapter_data.values()]
    acc_vals = [accuracies.get(name.split(' ')[0].lower(), 0) for name in names]
    
    scatter = ax1.scatter(params, acc_vals, s=150, c=range(len(names)), cmap='viridis', edgecolors='black')
    for i, name in enumerate(names):
        ax1.annotate(name.split(' ')[0], (params[i], acc_vals[i]), textcoords="offset points", xytext=(0,10), ha='center')
    ax1.set_xlabel('Parameters (Millions)'); ax1.set_ylabel('Accuracy'); ax1.set_title('A) Parameter Efficiency')

    gflop_cols = [col for col in df.columns if col.startswith('gflops_')]
    if gflop_cols:
        gflop_means = [df[col].mean() for col in gflop_cols]
        gflop_labels = [c.replace('gflops_', '').upper() for c in gflop_cols]
        ax2.bar(gflop_labels, gflop_means, color=COLORS[:len(gflop_labels)])
        ax2.set_ylabel('GFLOPs'); ax2.set_title('B) Computational Complexity')

    efficiency_scores = [acc / p for acc, p in zip(acc_vals, params) if p > 0]
    efficiency_labels = [n.split(' ')[0] for n in names]
    ax3.barh(efficiency_labels, efficiency_scores, color=COLORS[:len(efficiency_labels)])
    ax3.set_xlabel('Efficiency Score (Accuracy/M_Params)'); ax3.set_title('C) Parameter Efficiency Ranking')

    plt.tight_layout(pad=3.0)
    plt.savefig(PLOTS_DIR / "plot_adapter_performance.png", dpi=300)
    plt.close()
    print("   - Adapter performance plots generated.")

def plot_confidence_visualizations(df: pd.DataFrame):
    """Generates and saves visualizations for confidence analysis."""

    if not any(f"pred_{attr}_confidence" in df.columns for attr in ATTRS):
        print("   - Skipping confidence plots: No confidence data available.")
        return

    fig, axes = plt.subplots(2, 2, figsize=(12, 10), constrained_layout=True)
    for i, attr in enumerate(ATTRS):
        ax = axes.flatten()[i]
        conf_col = f"pred_{attr}_confidence"
        if conf_col in df.columns:
            sns.histplot(df, x=conf_col, hue=f"correct_{attr}", multiple="stack", ax=ax, palette=[COLORS[2], COLORS[0]])
            ax.set_title(f'Confidence Distribution: {attr.upper()}')
    plt.savefig(PLOTS_DIR / "plot_confidence_distributions.png", dpi=300)
    plt.close()
    print("   - Confidence visualization plots generated.")

# =============================================================================
# D3: Spatial Cross Heatmaps
# =============================================================================

def plot_spatial_cross_heatmaps(df: pd.DataFrame):
    """Generate spatial heat-maps showing crossing vs not-crossing behavior."""

    fig, axes = plt.subplots(1, 2, figsize=(10, 4))

    y_min, y_max = df["pred_cy"].min(), df["pred_cy"].max()
    x_min, x_max = 0, 1920

    cfg = [("crossing", axes[0]), ("not_crossing", axes[1])]
    for label, ax in cfg:
        subset = df[df["pred_cross"] == label]
        sns.kdeplot(x=subset["pred_cx"], y=subset["pred_cy"], fill=True,
                    cmap="Reds" if label == "crossing" else "Blues", ax=ax)
        ax.set_title(label)
        ax.set_xlim(0, 1920)
        ax.set_ylim(1080, 0)
        ax.set_xlabel("x (px)")
        ax.set_ylabel("y (px)")
    plt.suptitle("Spatial Distribution of Crossing vs Not-Crossing")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "plot_spatial_cross_heatmap.png", dpi=300)
    plt.close()
    print("   - Spatial heat-maps saved.")

# =============================================================================
# D4: Initial State Analysis
# =============================================================================

def initial_state_analysis(df: pd.DataFrame):
    """Generates and saves a plot of initial looking behavior vs. crossing intention."""
    first_frames = df.sort_values("frame_id").groupby(["video_id", "inference_track_id"]).head(30)
    stats = first_frames.groupby("gt_cross")["pred_look"].apply(lambda s: (s == "looking").mean()).reset_index(name="look_rate")
    
    plt.figure(figsize=(6, 5))
    sns.barplot(x="gt_cross", y="look_rate", data=stats, palette=COLORS, width=0.6)
    plt.ylabel("Fraction of Time Spent Looking (First 30 Frames)")
    plt.xlabel("Ground Truth Crossing Label")
    plt.title("Initial Gaze Behavior vs. Crossing Intention")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "plot_initial_gaze_vs_crossing.png", dpi=300)
    plt.close()
    print("   - Initial state analysis plot generated.")

# =============================================================================
# D5: Individual Pedestrian Gaze Tracking
# =============================================================================

def individual_pedestrian_gaze_tracking(df):
    """Track pedestrians' gaze behavior comparing partial vs no occlusion with dual analysis."""
    
    def process_occlusion_group_frames(df, occlusion_type, max_frames=150):
        """Process occlusion group and return frame-based statistics."""

        tracks = df[(df["gt_occlusion"] == occlusion_type) & 
                   (df["pred_look"].notna())].copy()
        
        if tracks.empty:
            return None, None, None, None
        
        track_groups = tracks.groupby(["video_id", "inference_track_id"])
        
        valid_tracks = []
        for (vid, track_id), group in track_groups:
            if len(group) >= 5:
                group_sorted = group.sort_values("frame_id").reset_index(drop=True)
                
                crossing_ratio = (group_sorted["pred_cross"] == "crossing").mean()
                if crossing_ratio > 0.5:
                    group_sorted["is_looking"] = (group_sorted["pred_look"] == "looking").astype(int)
                    valid_tracks.append(group_sorted)
        
        if not valid_tracks:
            return None, None, None, None
        
        track_matrix = []
        for track_df in valid_tracks:
            frames_to_take = min(max_frames, len(track_df))
            track_data = track_df.iloc[:frames_to_take]["is_looking"].values.astype(float)
            
            if len(track_data) < max_frames:
                padding = np.full(max_frames - len(track_data), np.nan)
                track_data = np.concatenate([track_data, padding])
            
            track_matrix.append(track_data)
        
        track_matrix = np.array(track_matrix)
        
        with np.errstate(invalid='ignore'):
            avg_looking = np.nanmean(track_matrix, axis=0)
            std_looking = np.nanstd(track_matrix, axis=0)
        
        valid_counts = np.sum(~np.isnan(track_matrix), axis=0)
        
        return avg_looking, std_looking, valid_counts, len(valid_tracks)
    
    def process_occlusion_group_progress(df, occlusion_type, max_frames=150):
        """Process occlusion group and return progress-based statistics."""

        tracks = df[(df["gt_occlusion"] == occlusion_type) & 
                   (df["pred_look"].notna())].copy()
        
        if tracks.empty:
            return None, None, None, None, None
        
        track_groups = tracks.groupby(["video_id", "inference_track_id"])
        
        valid_tracks = []
        for (vid, track_id), group in track_groups:
            if len(group) >= 5:
                group_sorted = group.sort_values("frame_id").reset_index(drop=True)
                
                crossing_ratio = (group_sorted["pred_cross"] == "crossing").mean()
                if crossing_ratio > 0.5:
                    group_sorted["is_looking"] = (group_sorted["pred_look"] == "looking").astype(int)
                    valid_tracks.append(group_sorted)
        
        if not valid_tracks:
            return None, None, None, None, None
        
        progress_bins = 20
        bin_edges = np.linspace(0, 1, progress_bins + 1)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        
        progress_matrix = []
        for track_df in valid_tracks:
            track_length = len(track_df)
            
            progress_values = np.linspace(0, 1, track_length)
            
            binned_looking = []
            for i in range(progress_bins):
                in_bin = (progress_values >= bin_edges[i]) & (progress_values < bin_edges[i + 1])
                if i == progress_bins - 1:
                    in_bin = (progress_values >= bin_edges[i]) & (progress_values <= bin_edges[i + 1])
                
                if np.any(in_bin):
                    bin_avg = track_df.iloc[in_bin]["is_looking"].mean()
                    binned_looking.append(bin_avg)
                else:
                    binned_looking.append(np.nan)
            
            progress_matrix.append(binned_looking)
        
        progress_matrix = np.array(progress_matrix)
        
        with np.errstate(invalid='ignore'):
            avg_looking = np.nanmean(progress_matrix, axis=0)
            std_looking = np.nanstd(progress_matrix, axis=0)
        
        valid_counts = np.sum(~np.isnan(progress_matrix), axis=0)
        
        return avg_looking, std_looking, valid_counts, len(valid_tracks), bin_centers
    
    max_frames = 200
    
    # Process both analyses
    partial_frames = process_occlusion_group_frames(df, "partial_occlusion", max_frames)
    no_occl_frames = process_occlusion_group_frames(df, "no_occlusion", max_frames)
    
    partial_progress = process_occlusion_group_progress(df, "partial_occlusion", max_frames)
    no_occl_progress = process_occlusion_group_progress(df, "no_occlusion", max_frames)
    
    if (partial_frames[0] is None and no_occl_frames[0] is None) or \
       (partial_progress[0] is None and no_occl_progress[0] is None):
        print("[skip] No crossing tracks found for either occlusion group.")
        return
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # ================================
    # TOP PLOT: Frame-based analysis
    # ================================
    
    if partial_frames[0] is not None:
        partial_avg_f, partial_std_f, partial_counts_f, partial_n_f = partial_frames
        valid_frames_partial = partial_counts_f >= 1
        frame_indices_partial = np.arange(max_frames)[valid_frames_partial]
        avg_looking_partial_f = partial_avg_f[valid_frames_partial]
        std_looking_partial_f = partial_std_f[valid_frames_partial]
        
        window_size = 5
        if len(avg_looking_partial_f) >= window_size:
            padded_data = np.pad(avg_looking_partial_f, (window_size//2, window_size//2), mode='edge')
            smoothed_partial_f = np.convolve(padded_data, np.ones(window_size)/window_size, mode='valid')
            padded_std = np.pad(std_looking_partial_f, (window_size//2, window_size//2), mode='edge')
            smoothed_std_f = np.convolve(padded_std, np.ones(window_size)/window_size, mode='valid')
        else:
            smoothed_partial_f = avg_looking_partial_f
            smoothed_std_f = std_looking_partial_f
        
        ax1.plot(frame_indices_partial, smoothed_partial_f, 
                 color=COLORS[1], linewidth=3, marker='o', markersize=3,
                 label=f'Partial Occlusion (n={partial_n_f} pedestrians)')
        
        ax1.fill_between(frame_indices_partial, 
                         np.maximum(0, smoothed_partial_f - smoothed_std_f),
                         np.minimum(1, smoothed_partial_f + smoothed_std_f),
                         alpha=0.2, color=COLORS[1])
    
    if no_occl_frames[0] is not None:
        no_occl_avg_f, no_occl_std_f, no_occl_counts_f, no_occl_n_f = no_occl_frames
        valid_frames_no_occl = no_occl_counts_f >= 1
        frame_indices_no_occl = np.arange(max_frames)[valid_frames_no_occl]
        avg_looking_no_occl_f = no_occl_avg_f[valid_frames_no_occl]
        std_looking_no_occl_f = no_occl_std_f[valid_frames_no_occl]
        
        ax1.plot(frame_indices_no_occl, avg_looking_no_occl_f, 
                 color=COLORS[0], linewidth=3, marker='s', markersize=3,
                 label=f'No Occlusion (n={no_occl_n_f} pedestrians)')
        
        ax1.fill_between(frame_indices_no_occl, 
                         np.maximum(0, avg_looking_no_occl_f - std_looking_no_occl_f),
                         np.minimum(1, avg_looking_no_occl_f + std_looking_no_occl_f),
                         alpha=0.2, color=COLORS[0])
    
    ax1.set_xlabel("Frame Index in Track")
    ax1.set_ylabel("Average Looking Rate")
    ax1.set_title("A) Frame-Based Analysis (Detection Window)")
    ax1.set_xlim(0, max_frames)
    ax1.set_ylim(-0.1, 1.2)
    ax1.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    ax1.text(0.02, 0.98, "Note: Frame 0 = first detection, not crossing start", 
             transform=ax1.transAxes, fontsize=9, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
    
    # ================================
    # BOTTOM PLOT: Progress-based analysis
    # ================================
    
    if partial_progress[0] is not None:
        partial_avg_p, partial_std_p, partial_counts_p, partial_n_p, partial_prog = partial_progress
        valid_bins_partial = partial_counts_p >= 1
        progress_partial = partial_prog[valid_bins_partial] * 100
        avg_looking_partial_p = partial_avg_p[valid_bins_partial]
        std_looking_partial_p = partial_std_p[valid_bins_partial]
        
        window_size = 3
        if len(avg_looking_partial_p) >= window_size:
            padded_data = np.pad(avg_looking_partial_p, (window_size//2, window_size//2), mode='edge')
            smoothed_partial_p = np.convolve(padded_data, np.ones(window_size)/window_size, mode='valid')
            padded_std = np.pad(std_looking_partial_p, (window_size//2, window_size//2), mode='edge')
            smoothed_std_p = np.convolve(padded_std, np.ones(window_size)/window_size, mode='valid')
        else:
            smoothed_partial_p = avg_looking_partial_p
            smoothed_std_p = std_looking_partial_p
        
        ax2.plot(progress_partial, smoothed_partial_p, 
                 color=COLORS[1], linewidth=3, marker='o', markersize=3,
                 label=f'Partial Occlusion (n={partial_n_p} pedestrians)')
        
        ax2.fill_between(progress_partial, 
                         np.maximum(0, smoothed_partial_p - smoothed_std_p),
                         np.minimum(1, smoothed_partial_p + smoothed_std_p),
                         alpha=0.2, color=COLORS[1])
    
    if no_occl_progress[0] is not None:
        no_occl_avg_p, no_occl_std_p, no_occl_counts_p, no_occl_n_p, no_occl_prog = no_occl_progress
        valid_bins_no_occl = no_occl_counts_p >= 1
        progress_no_occl = no_occl_prog[valid_bins_no_occl] * 100
        avg_looking_no_occl_p = no_occl_avg_p[valid_bins_no_occl]
        std_looking_no_occl_p = no_occl_std_p[valid_bins_no_occl]
        
        ax2.plot(progress_no_occl, avg_looking_no_occl_p, 
                 color=COLORS[0], linewidth=3, marker='s', markersize=3,
                 label=f'No Occlusion (n={no_occl_n_p} pedestrians)')
        
        ax2.fill_between(progress_no_occl, 
                         np.maximum(0, avg_looking_no_occl_p - std_looking_no_occl_p),
                         np.minimum(1, avg_looking_no_occl_p + std_looking_no_occl_p),
                         alpha=0.2, color=COLORS[0])
    
    ax2.set_xlabel("Crossing Progress (%)")
    ax2.set_ylabel("Average Looking Rate")
    ax2.set_title("B) Progress-Based Analysis (Normalized Crossing Stages)")
    ax2.set_xlim(0, 100)
    ax2.set_ylim(-0.1, 1.2)
    ax2.set_xticks([0, 20, 40, 60, 80, 100])
    ax2.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "plot_gaze_comparison_dual_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   - Dual gaze comparison plot saved.")
    print(f"    Frame-based analysis: {partial_n_f if partial_frames[0] is not None else 0} partial, {no_occl_n_f if no_occl_frames[0] is not None else 0} no-occlusion")
    print(f"    Progress-based analysis: {partial_n_p if partial_progress[0] is not None else 0} partial, {no_occl_n_p if no_occl_progress[0] is not None else 0} no-occlusion")


# =============================================================================
# E. MAIN EXECUTION FLOW
# =============================================================================
def main(csv_path: str):
    """Main execution function to run all analysis and visualization components."""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Input CSV file not found at: {csv_path}")

    PLOTS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"Starting analysis of '{csv_path}'. Outputs will be saved to '{PLOTS_DIR}/'")
    
    df = load_data(csv_path)
    print("Data loaded and preprocessed successfully.")

    print("\n--- Generating Analysis Reports (Markdown) ---")
    basic_stats(df)
    detailed_classification_metrics(df)
    comprehensive_performance_analysis(df)
    model_confidence_analysis(df)
    advanced_adapter_analysis(df)

    print("\n--- Generating Visualizations (PNG) ---")
    plot_latency_and_scalability(df)
    plot_adapter_performance_visualizations(df)
    plot_confidence_visualizations(df)
    plot_spatial_cross_heatmaps(df)
    initial_state_analysis(df)
    individual_pedestrian_gaze_tracking(df)

    print("\nAll analysis complete.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze raw inference data for pedestrian behavior and generate reports and plots.")
    parser.add_argument(
        "--csv", 
        type=str, 
        default=os.path.join("Generated_Data", "raw_data_pedestrian_behavior_step2.csv"),
        help="Path to the raw inference CSV file from Step 2."
    )
    args = parser.parse_args()
    main(args.csv)
